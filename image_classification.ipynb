{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fa8486-9e10-459a-b7b8-6421f4b323f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  lib.project_functions import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a21d4-2162-4ed9-bb46-70c84fec890d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train, val = generator(train=True, validation=True, generator_batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bffdbba-ddda-4e62-9906-43919d713e7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sample = next(train)\n",
    "\n",
    "print(train_sample[0].shape) \n",
    "print(train_sample[1].shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d36f84a-489f-486c-8118-735ac48dc1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sample = next(val)\n",
    "\n",
    "print(val_sample[0].shape) \n",
    "print(val_sample[1].shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bcb3b5-4bcd-40aa-94c2-042c4e7f0972",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0e3d30-6c17-4bf0-9d05-af260250e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "convnext_tiny  = tf.keras.applications.convnext.ConvNeXtTiny(\n",
    "                    model_name='convnext_tiny',\n",
    "                    include_top=False,\n",
    "                    include_preprocessing=True,\n",
    "                    weights='imagenet',\n",
    "                    input_tensor=None,\n",
    "                    input_shape=(32, 32, 3) )\n",
    "\n",
    "print( f\"len -> {len(convnext_tiny.layers)}\\ntype -> {type(convnext_tiny.layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f311ee1-851f-4378-bbca-3e33f6e28c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in convnext_tiny.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "for layer in convnext_tiny.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35534e8-4060-4ade-9759-f2dbdd64bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convnext_tiny.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe352d8-7227-47a8-9adc-4c0f8ee95362",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = int(pd.read_csv(\"data/Train.csv\", usecols= [\"ClassId\"]).nunique())\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e106adcb-52a7-4d8a-8e14-902b0dc97765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc0bf9-8119-46e4-9f66-ac3b19693476",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(32, 32, 3))\n",
    "\n",
    "convnext = convnext_tiny(inputs)\n",
    "print(\"Shape of time_distributed_output:\", convnext.shape)\n",
    "\n",
    "flattened = Flatten()(convnext)\n",
    "\n",
    "dance = layers.Dense(1024, activation='relu')(flattened)\n",
    "dance = layers.Dense(512, activation='relu')(dance)\n",
    "dance = layers.Dense(512, activation=\"relu\")(dance)\n",
    "print(\"Shape of dance:\", dance.shape)\n",
    "\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(dance)\n",
    "\n",
    "print(\"Shape of outputs:\", outputs.shape)\n",
    "\n",
    "convnext_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "convnext_model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "              metrics=['accuracy', f1])\n",
    "\n",
    "callback = EarlyStopping(\n",
    "    patience=20,\n",
    "    monitor=\"val_f1\",\n",
    "    restore_best_weights=True,\n",
    "    min_delta=0.001\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath = \"modelcheckpoint/image_model.h5\",  \n",
    "    monitor='val_f1', \n",
    "    verbose=1,  \n",
    "    save_best_only=True,  \n",
    "    mode='min'  \n",
    ")\n",
    "\n",
    "\n",
    "hist = convnext_model.fit(train, epochs=1000, validation_data=val, verbose=1,\n",
    "          callbacks=[callback, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3596043b-ba1b-40e9-99e2-ff4fa4ffc07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(32, 32, 3))\n",
    "\n",
    "conv1 = Conv2D(32, (3, 3),  padding=\"same\")(inputs)\n",
    "conv1 = Activation('relu')(conv1)\n",
    "conv1 = Conv2D(64, (3, 3),  padding=\"same\")(conv1)\n",
    "conv1 = Activation('relu')(conv1)\n",
    "conv1 = Conv2D(128, (3, 3),  padding=\"same\")(conv1)\n",
    "conv1 = BatchNormalization()(conv1)\n",
    "conv1 = Activation('relu')(conv1)\n",
    "dropout1 = Dropout(0.3)(conv1)\n",
    "pooling1 = MaxPooling2D()(dropout1)\n",
    "\n",
    "\n",
    "conv2 = Conv2D(256, (3, 3),  padding=\"same\")(pooling1)\n",
    "conv2 = Activation('relu')(conv2)\n",
    "conv2 = Conv2D(512, (3, 3),  padding=\"same\")(conv2)\n",
    "conv2 = Activation('relu')(conv2)\n",
    "conv2 = Conv2D(512, (3, 3),  padding=\"same\")(conv2)\n",
    "conv2 = BatchNormalization()(conv2)\n",
    "conv2 = Activation('relu')(conv2)\n",
    "dropout2 = Dropout(0.2)(conv2)\n",
    "pooling2 = MaxPooling2D()(dropout2)\n",
    "\n",
    "flattened = Flatten()(pooling2)\n",
    "outputs = Dense(units=43, activation=\"softmax\")(flattened)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[f1])\n",
    "\n",
    "hist = model.fit(train, epochs=1000, validation_data=val, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c434e4-5a4b-44ba-905a-6f6aa70c4f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ad2899e-72b3-48ef-a2ab-cd8883d178e9",
   "metadata": {},
   "source": [
    "# VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af098e3-ddb4-4f51-9d7c-1a44b8cb717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Загрузка предварительно обученной модели VGG16 без верхних слоев (include_top=False)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "\n",
    "# Добавление своего классификатора поверх базовой модели\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Создание окончательной модели\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Заморозка весов базовой модели\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Компиляция модели\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=[f1])\n",
    "\n",
    "# Вывод структуры модели\n",
    "# model.summary()\n",
    "\n",
    "model.fit(\n",
    "        train,\n",
    "        steps_per_epoch=len(train),\n",
    "        epochs=1000,\n",
    "        validation_data=val,\n",
    "        validation_steps=len(val),\n",
    "        callbacks=[callback, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9800e01-a167-46ba-893a-a3b980038154",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smmery = summarize_diagnostics(hist, save_path= \"modelcheckpoint/image_model.h5.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910be571-2218-458f-9e81-e4e1cd1fb50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(\"modelcheckpoint/image_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808ba75-b88d-48b7-a6a2-310f4bc0f5e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_evaluate = loaded_model.evaluate(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365042a-1cfb-4324-9427-31c3b38269e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da7b3fb7-89c4-4aef-aea6-696f8e9afa69",
   "metadata": {},
   "source": [
    "# Test on my data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5753c9-e942-471d-a452-f233bb8affed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_foto(show = None):\n",
    "    data_path = \"data\\MyFoto\"\n",
    "    my_foto_names = os.listdir(data_path)\n",
    "    images = []\n",
    "    \n",
    "    for foto_name in my_foto_names:\n",
    "        path = os.path.join(data_path, foto_name)\n",
    "       \n",
    "        image_cv = cv.imread(path)\n",
    "        image_cv = cv.cvtColor(image_cv, cv.COLOR_BGR2RGB)\n",
    "        # image_cv = image_preprocessing(image_cv)\n",
    "        # resize image\n",
    "        image_cv = cv.resize(image_cv, ( 32, 32), interpolation=cv.INTER_LINEAR)\n",
    "        image_cv = image_cv / 255.0\n",
    "        \n",
    "        if show:\n",
    "            image_show(image_cv)\n",
    "            \n",
    "        image_cv = np.expand_dims(image_cv, axis=0)  \n",
    "        images.append(image_cv)\n",
    "    \n",
    "    return images\n",
    "\n",
    "def predict(images, model ):\n",
    "    predicted_classes = []\n",
    "    for image in images:\n",
    "        predicted_probabilities = model.predict(image)\n",
    "        predicted_classes.append(np.argmax(predicted_probabilities, axis=1))\n",
    "    return predicted_classes\n",
    "\n",
    "images = my_foto( show = True)\n",
    "# predicted = predict(images, loaded_model)\n",
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd42387e-4526-44ac-9490-ec6295bf9ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915dd16-2b17-4943-8f69-51979afef55b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = \"data\\Meta\"\n",
    "meta = os.listdir(data_path)\n",
    "\n",
    "for klass, m, image in zip(predicted, meta, images):\n",
    "    num_im_m = str(klass.item()) + \".png\"\n",
    "    path = os.path.join(data_path, num_im_m)\n",
    "    image_2 = cv.imread(path)\n",
    "    image_2 = cv.cvtColor(image_2, cv.COLOR_BGR2RGB)\n",
    "    \n",
    "\n",
    "    image_restored = np.squeeze(image, axis=0)  # Удаление добавленного измерения\n",
    "    \n",
    "    print(klass)\n",
    "    image_show(image_2)\n",
    "    image_show(image_restored)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca3bf56-2fda-4538-a024-b34338e10132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7301f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd3779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcc964c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2fa71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5147c66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7922aa49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864d362f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f8f613-b442-481b-a419-0b82051420bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e405ed7d-e2a9-4591-a47f-56eff9cabb67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa64196-6566-46c6-a0e7-6e71e89ec4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d5133d-0b68-44f4-8797-da566821ed9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf1f3d1-fffc-4e6b-a9cf-04ada3fe01c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97016e36-2a35-46fb-89ac-9c5315ea8b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b2b135-f54d-4ff5-9858-2037bafbf29a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c161d232-09ac-4392-9772-f2db7c4d9f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f18bd3-fab1-48a9-b55e-5efe72e98237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a18e7eb-8a3a-4700-966e-4add7f47a2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc829f9-4065-4af8-a93e-65227701fb2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7872ddee-9823-4a1a-b83d-791e1b6a24cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a44020-a181-495f-a655-745beb309039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a513f5d1-6673-4634-99ce-e30ffec1549f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a9398-2f09-46e2-a6f4-07a9f1bd8cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b532b1e2-5fad-4e92-89ab-b7f7dc9e51ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79105360-ada2-4083-b837-a4e3407b25ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a4f0d-acae-4f1f-ae73-b70297574bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7d538b-65b7-4638-a4d8-c9bff007734a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab33ef3-9c60-4ba5-9024-47303eea0691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7f46ad-eed6-450a-98b7-9813de183b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898bc753-1322-4e19-a199-2dc43eae2b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a6002-2b42-4fb7-b806-973e2365126a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd0509-bb28-4dc6-862a-3fa2cefe54fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d9488-33cd-4f4b-8695-01e4f9cc10c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d80b2fd-6059-44c7-a5dc-7936cc5d96e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683f0be8-f636-4a03-a16b-fb6c7cacf67f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508dafa5-70a5-4677-968d-cba20dfc755a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb9ae30-37cf-48ec-8841-ce846c8cc959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28fac50-7e72-4c20-a003-3eafbbc59e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c6225-3d15-43c4-a1e8-3ba589646862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cbbfb3-a394-4555-8d71-bfc226af53e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e127ca19-58dc-4c9d-9140-5c8693841132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f9fbcb-8d23-4556-b57f-28c2f7d7e435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb47569-1884-4cb2-ae47-c22b6789f5da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f6ce83-6de1-4f68-9f9d-304eede3f2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
